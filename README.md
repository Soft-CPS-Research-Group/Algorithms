# Algorithms for Energy Flexibility Optimization

This repository hosts the training side of the energAIze platform. It runs
multiâ€‘agent reinforcement learning experiments on
[CityLearn](https://github.com/intelligent-environments-lab/CityLearn), exports
ONNX policies plus metadata, and feeds the
[`energAIze_inference`](https://github.com/your-org/energAIze_inference) serving
layer. Students extend the *algorithms* while the platform handles
orchestration, logging, and packaging.

## Documentation Map

- ğŸ“˜ [Training Platform Guide](docs/platform_guide.md): architecture, config
  schema, extension workflow, troubleshooting.
- ğŸ“„ [Manifest example](docs/examples/manifest_example.json): sample
  `artifact_manifest.json` emitted after training.
- ğŸ”Œ [Inference bundle contract](docs/inference_bundle.md): files the serving API
  expects and how to load them.

## Prerequisites

- Python 3.10
- `pip install -r requirements.txt`
- (Optional) `mlflow` if you want the UI locally
- Docker (optional, for containerised runs)

## Quick Start

### Local training

```bash
python run_experiment.py --config configs/config.yaml --job-id dev-run
```

Outputs are written to `./runs/`:

- `runs/jobs/<job_id>/` â€“ logs, checkpoints, ONNX exports, manifest.
- `runs/mlflow/mlruns/` â€“ MLflow tracking store (serve the UI with
  `mlflow ui --backend-store-uri file:./runs/mlflow/mlruns`).

### Inside Docker

The Docker image sets `OPEVA_BASE_DIR=/data`. Mount a volume at `/data` and run
with the same arguments:

```bash
python run_experiment.py --config /data/configs/<experiment>.yaml --job-id <job_id>
```

Artefacts appear under `/data/jobs/<job_id>/`, ready to publish or archive for
inference.

## Repository Layout

- `run_experiment.py` â€“ entrypoint used both locally and in Docker.
- `algorithms/agents/` â€“ algorithm implementations; every class inherits
  `BaseAgent`.
- `algorithms/constants.py` â€“ shared constants (e.g., ONNX opset pin).
- `utils/wrapper_citylearn.py` â€“ wrapper around CityLearn (encoders, logging,
  checkpoints, manifest metadata).
- `configs/` â€“ experiment configs and encoder rules (`configs/encoders/*.json`).
- `reward_function/` â€“ custom reward implementations.
- `tests/` â€“ unit tests for encoders, config validation, etc.
- `docs/` â€“ platform guide, inference bundle spec, manifest example.
- `runs/` â€“ generated at runtime to mirror the `/data` mount.

## Training Flow at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YAML Config(s) â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ run_experiment.py (entry)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ - validates config         â”‚
                         â”‚ - boots MLflow + logging   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Wrapper_CityLearn          â”‚
                        â”‚ - builds encoders from     â”‚
                        â”‚   configs/encoders/*.json  â”‚
                        â”‚ - drives CityLearn loop    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                           â”‚
                     â–¼                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ CityLearn Simulator        â”‚   â”‚ BaseAgent (MADDPG) â”‚
        â”‚ datasets/<schema>.json     â”‚   â”‚ algorithms/agents  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                            â”‚
                     â–¼                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Training Artefacts         â”‚   â”‚ MLflow Tracking    â”‚
        â”‚ runs/jobs/<job_id>/        â”‚   â”‚ runs/mlflow/mlruns â”‚
        â”‚ â”œâ”€ checkpoints/            â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ â”œâ”€ onnx_models/            â”‚
        â”‚ â””â”€ artifact_manifest.json  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ exported bundle
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ energAIze_inference service (separate repo) â”‚
        â”‚ consumes manifest + ONNX per agent          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Use this diagram as the mental model when wiring CI or new algorithms.

## Configuration Essentials

- Base config lives in `configs/config.yaml` (students typically copy this).
- Validation happens in `utils/config_schema.py` (Pydantic models).
- Observation encoders are defined in `configs/encoders/default.json` so training
  and inference stay aligned.
- Templates under `configs/templates/*.yaml` provide starting points for new
  algorithms/datasets.

Important knobs:

- `tracking.mlflow_enabled` â€“ switch between MLflow and JSONL logging.
- `tracking.log_frequency` â€“ log rewards/system metrics every N steps (default 1).
- `checkpointing.resume_training` â€“ reload agent state from MLflow (simulator
  still restarts).
- `training.*` â€“ exploration warm-up, update cadence, target refresh.
- `algorithm.*` â€“ hyperparameters, network sizes, replay buffer type, exploration
  policy. `RuleBasedPolicy` replaces neural network knobs with PV thresholds,
  flexibility buffers, and charger overrides.

## Training Outputs & Inference Hand-off

Every job produces:

- `jobs/<job_id>/logs/<run_id>.log` â€“ Loguru trace.
- `jobs/<job_id>/progress/progress.json` â€“ progress updates for dashboards.
- `jobs/<job_id>/results/result.json` â€“ CityLearn KPI pivot table.
- `jobs/<job_id>/checkpoints/` â€“ optional checkpoints.
- `jobs/<job_id>/onnx_models/` â€“ one ONNX actor per agent.
- `jobs/<job_id>/artifact_manifest.json` â€“ metadata consumed by the inference
  repo.
- `jobs/<job_id>/rbc_policy.json` â€“ exported schedule and heuristics when
  `RuleBasedPolicy` runs.

Bundle the manifest, ONNX directory, and optional alias map as described in
[`docs/inference_bundle.md`](docs/inference_bundle.md) to deploy a trained agent.

## Extending the Platform (Students)

1. Implement an agent under `algorithms/agents/` inheriting `BaseAgent`.
2. Register it in `algorithms/registry.py`.
3. Honour the full `BaseAgent.update` signature (scheduler flags are
   keyword-only and mandatory).
4. Extend `configs/templates/` and `utils/config_schema.py` if new
   hyperparameters are needed.
5. Implement `export_artifacts` so ONNX + metadata are written under the run
   directory. Use the shared `DEFAULT_ONNX_OPSET` (13).
6. Add unit tests (`pytest`) for new utilities or encoders.

The [Training Platform Guide](docs/platform_guide.md) walks through this process
in detail and contains troubleshooting tips.

## Monitoring & Logging

- MLflow records per-step metrics according to `tracking.log_frequency` when enabled.
- If MLflow is disabled (`tracking.mlflow_enabled: false`), metrics go to
  `<log_dir>/metrics.jsonl` via `LocalMetricsLogger`.
- Wrapper_CityLearn samples CPU/RAM/GPU utilisation every 10 steps.
- Use Loguru levels consistently (`info` for progress, `debug` for verbose state,
  `warning` for recoverable issues).

## Contributing

1. Create a virtual environment and install dependencies (`pip install -r requirements.txt`).
2. Run a smoke test: `python run_experiment.py --config configs/config.yaml --job-id smoke-test`.
3. Execute the test suite: `pytest`.
4. Submit focused PRs with documentation updates when behaviour changes.

## FAQ

- **Can I resume mid-episode?** No. Checkpoint resume reloads the agent state but
  CityLearn restarts at the beginning of the episode.
- **How do I disable MLflow logging?** Set `tracking.mlflow_enabled: false`.
  Metrics stream to `<log_dir>/metrics.jsonl`.
- **Where do I inspect preprocessing metadata?** Open
  `jobs/<job_id>/artifact_manifest.json`; it lists encoders, action bounds,
  reward configuration, and ONNX artefacts.

## Roadmap

- Capture CityLearn environment state for true mid-episode resume.
- Extend schema/templates for additional algorithm families.
- Provide inference-side manifest validation scripts.
- Publish tutorial notebooks and CI integration examples.

## License

MIT
