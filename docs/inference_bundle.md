# Inference Bundle Contract

The inference service (`energAIze_inference`) expects each trained job to publish a self-contained bundle with the following layout:

```
<job_bundle>/
  artifact_manifest.json   # metadata describing everything below
  onnx_models/
    agent_0.onnx
    agent_1.onnx
  aliases.json             # OPTIONAL – feature alias map if runtime names differ (flat JSON)
```

## Required Files

### `artifact_manifest.json`
Generated by `utils.artifact_manifest.write_manifest`. Key fields consumed by the API:

- `manifest_version`: must be `1`.
- `metadata.experiment_name` / `metadata.run_name`: surfaced in `/info`.
- `topology`
  - `num_agents`
  - `observation_dimensions`
  - `action_dimensions`
- `environment`
  - `observation_names`: ordered list per agent.
  - `encoders`: array matching `observation_names` describing encoder type + parameters (used to rebuild preprocessing).
  - `action_bounds`: per-agent `[low, high]` vectors for output clamping.
  - `action_names`: optional, returned by the inference API when present.
  - `reward_function`: `{ "name": str, "params": {...} }` – drives `/reward` endpoint.
- `agent`
  - `format`: currently `onnx`.
  - `artifacts`: list of `{agent_index, path, observation_dimension, action_dimension}`. `path` is relative to the manifest directory.

Any additional metadata returned by the agent (e.g., exploration statistics) is preserved but ignored by the current inference stack.

### `onnx_models/`
One ONNX graph per agent index, matching the entries in `artifact_manifest.json`. The inference service loads them via ONNX Runtime CPU provider. Keep opset ≤ `DEFAULT_ONNX_OPSET` (currently 13).

### Optional `aliases.json`
When feature names in production differ from those in the manifest, supply a flat alias map the inference service can read:

```json
{
  "temperature": "indoor_air_temp",
  "hvac_kw": "hvac_power_kw"
}
```

The API rewrites incoming features using this mapping before applying encoders.

## Deployment Flow

1. **Training job finishes**: `run_experiment.py` writes `artifact_manifest.json` and ONNX files under `runs/jobs/<job_id>/`.
2. **Bundle packaging**: copy (or archive) the manifest, ONNX directory, and optional alias file.
3. **Inference load**: send the bundle location to the inference admin endpoint:
   ```bash
   curl -X POST http://<host>/admin/load \
     -H "Content-Type: application/json" \
     -d '{
           "manifest_path": "/data/jobs/<job_id>/artifact_manifest.json",
           "agent_index": 0,
           "artifacts_dir": "/data/jobs/<job_id>/logs",
           "alias_mapping_path": "/data/jobs/<job_id>/aliases.json"
         }'
   ```
   - `artifacts_dir` should point to the root that contains `onnx_models/` (often the run log directory).
   - Call `/admin/load` once per agent container (set `MODEL_AGENT_INDEX` or use the request body).
4. **Inference calls**: clients post JSON payloads whose keys match `environment.observation_names` (after aliasing). The API returns action names/values defined in the manifest.

## Quick Checklist

- [ ] `artifact_manifest.json` present and valid JSON.
- [ ] ONNX files exist for every `agent.artifacts` entry (relative paths resolved).
- [ ] Opset ≤ 13 (use the shared `DEFAULT_ONNX_OPSET`).
- [ ] Optional `aliases.json` is a flat map when feature names change across deployments.
- [ ] Bundle stored somewhere the inference service can read (NFS, object store, etc.).

Share this document with anyone exporting models so they know exactly what the serving layer consumes.
