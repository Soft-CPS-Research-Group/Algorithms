# Minimal MADDPG template highlighting the knobs students typically tune. Copy
# this file and adjust seeds, network sizes, or exploration noise as needed.
metadata:
  experiment_name: "maddpg_template"
  run_name: "MADDPG Training"

runtime:
  log_dir: null
  mlflow_uri: null

tracking:
  mlflow_enabled: true
  log_level: "INFO"
  log_frequency: 1

checkpointing:
  resume_training: false
  checkpoint_run_id: null
  checkpoint_artifact: "latest_checkpoint.pth"
  use_best_checkpoint_artifact: false
  reset_replay_buffer: false
  freeze_pretrained_layers: false
  fine_tune: false
  checkpoint_interval: 10000

simulator:
  dataset_name: citylearn_challenge_2022_phase_all_plus_evs
  dataset_path: ./datasets/citylearn_challenge_2022_phase_all_plus_evs/schema.json
  central_agent: false
  reward_function: V2GPenaltyReward

training:
  seed: 123
  end_initial_exploration_time_step: 96
  end_exploration_time_step: 192
  steps_between_training_updates: 4
  target_update_interval: 2

topology:
  num_agents: null
  observation_dimensions: null
  action_dimensions: null
  action_space: null

algorithm:
  name: "MADDPG"
  hyperparameters:
    gamma: 0.995
  networks:
    actor:
      class: "Actor"
      layers: [1024, 512, 256]
      lr: 5.0e-5
    critic:
      class: "Critic"
      layers: [1024, 512, 256]
      lr: 5.0e-4
  replay_buffer:
    class: "MultiAgentReplayBuffer"
    capacity: 200000
    batch_size: 256
  exploration:
    strategy: "GaussianNoise"
    params:
      bias: 0.1
      sigma: 0.15
      decay: 0.99
      gamma: 0.99
      tau: 0.001
