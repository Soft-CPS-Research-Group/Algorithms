experiment:
  name: "test_experiment_1"

logging:
  mlflow: false

simulator:
  dataset_name: your_dataset_name         # Dataset name to load
  central_agent: false                    # Whether to use a central agent
  reward_function: V2GPenaltyReward       # Name of the reward function class
  num_episodes: 100
  max_steps: 200

hyperparameters:
  num_agents: 2
  gamma: 0.99
  batch_size: 128
  buffer_size: 100000
  tau: 0.001
  sigma: 0.2
  actor_units: [256, 128]
  critic_units: [256, 128]
  lr_actor: 1e-4
  lr_critic: 1e-3
  steps_between_training_updates: 5
  target_update_interval: 2

replay_buffer:
  class: "ReplayBuffer1"
  params:
    capacity: 100000
    num_agents: 2
    batch_size: 128

exploration:
  strategy: "GaussianNoise"
  params:
    sigma: 0.2
    decay: 0.995

algorithm:
  actor_network:
    class: "Actor"
    params:
      layers: [256, 128]
  critic_network:
    class: "Critic"
    params:
      layers: [256, 128]
  optimizer:
    actor:
      class: "Adam"
      params:
        lr: 1e-4
    critic:
      class: "Adam"
      params:
        lr: 1e-3